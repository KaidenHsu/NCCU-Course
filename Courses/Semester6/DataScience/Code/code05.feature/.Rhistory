iris$class <- as.numeric(iris$Species == "setosa")
set.seed(2345)
intrain <- runif(nrow(iris)) < 0.75
train <- iris[intrain,]
test <- iris[!intrain,]
head(train)
##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species class
## 1          5.1         3.5          1.4         0.2  setosa     1
## 2          4.9         3.0          1.4         0.2  setosa     1
## 3          4.7         3.2          1.3         0.2  setosa     1
## 4          4.6         3.1          1.5         0.2  setosa     1
## 5          5.0         3.6          1.4         0.2  setosa     1
## 6          5.4         3.9          1.7         0.4  setosa     1
source("lime_iris_example.R")
install.packages(xgboost)
install.packages("xgboost")
iris <- iris
iris$class <- as.numeric(iris$Species == "setosa")
set.seed(2345)
intrain <- runif(nrow(iris)) < 0.75
train <- iris[intrain,]
test <- iris[!intrain,]
head(train)
##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species class
## 1          5.1         3.5          1.4         0.2  setosa     1
## 2          4.9         3.0          1.4         0.2  setosa     1
## 3          4.7         3.2          1.3         0.2  setosa     1
## 4          4.6         3.1          1.5         0.2  setosa     1
## 5          5.0         3.6          1.4         0.2  setosa     1
## 6          5.4         3.9          1.7         0.4  setosa     1
source("lime_iris_example.R")
input <- as.matrix(train[, 1:4])
model <- fit_iris_example(input, train$class)
library(lime)
install.packages("lime")
iris <- iris
iris$class <- as.numeric(iris$Species == "setosa")
set.seed(2345)
intrain <- runif(nrow(iris)) < 0.75
train <- iris[intrain,]
test <- iris[!intrain,]
head(train)
##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species class
## 1          5.1         3.5          1.4         0.2  setosa     1
## 2          4.9         3.0          1.4         0.2  setosa     1
## 3          4.7         3.2          1.3         0.2  setosa     1
## 4          4.6         3.1          1.5         0.2  setosa     1
## 5          5.0         3.6          1.4         0.2  setosa     1
## 6          5.4         3.9          1.7         0.4  setosa     1
source("lime_iris_example.R")
input <- as.matrix(train[, 1:4])
model <- fit_iris_example(input, train$class)
library(lime)
explainer <- lime(train[,1:4],
model = model,
bin_continuous = TRUE,
n_bins = 10)
(example <- test[5, 1:4, drop=FALSE])
explanation <- lime::explain(example,
explainer,
n_labels = 1,
n_features = 4)
plot_features(explanation)
(example <- test[c(13, 24), 1:4])
test$class[c(13,24)]
explanation <- explain(example,
explainer,
n_labels = 1,
n_features = 4,
kernel_width = 0.5)
plot_features(explanation)
#
# Convenience function for running iris example from section 6.3.2
# of Practical Data Science with R, Second Edition
#
# if this fails, make sure xgboost is installed:
# install.packages("xgboost")
library(xgboost)
#
# Input:
# - variable_matrix: matrix of input data
# - labelvvec: numeric vector of class labels (1 is positive class)
#
# Returns:
# - xgboost model
#
fit_iris_example = function(variable_matrix, labelvec) {
cv = xgb.cv(variable_matrix, label = labelvec,
params=list(
objective="binary:logistic"
),
nfold=5,
nrounds=100,
print_every_n=10,
metrics="logloss")
evalframe = as.data.frame(cv$evaluation_log)
NROUNDS = which.min(evalframe$test_logloss_mean)
model = xgboost(data=variable_matrix, label=labelvec,
params=list(
objective="binary:logistic"
),
nrounds=NROUNDS,
verbose=FALSE)
model
}
#
# Convenience function for running iris example from section 6.3.2
# of Practical Data Science with R, Second Edition
#
# if this fails, make sure xgboost is installed:
# install.packages("xgboost")
library(xgboost)
#
# Input:
# - variable_matrix: matrix of input data
# - labelvec: numeric vector of class labels (1 is positive class)
#
# Returns:
# - xgboost model
#
fit_iris_example = function(variable_matrix, labelvec) {
cv = xgb.cv(variable_matrix, label = labelvec,
params=list(
objective="binary:logistic"
),
nfold=5,
nrounds=100,
print_every_n=10,
metrics="logloss")
evalframe = as.data.frame(cv$evaluation_log)
NROUNDS = which.min(evalframe$test_logloss_mean)
model = xgboost(data=variable_matrix, label=labelvec,
params=list(
objective="binary:logistic"
),
nrounds=NROUNDS,
verbose=FALSE)
model
}
x <- c(1, 4)
y <- c(2, 0)
length(x)
x+y
ls()
matrix(c(1,2,3,4),2,2)
seq(1,5)
a = seq(1,5)
a
print(a)
seq(1,10,length.out=5)
c(6,'fred')
list(6,'fred')
# Credit by thiagogm, https://www.r-bloggers.com/computing-and-visualizing-pca-in-r/
# Load data
data(iris)
head(iris, 3)
# log transform
log.ir <- log(iris[, 1:4])
ir.species <- iris[, 5]
# apply PCA - scale. = TRUE is highly advisable, but default is FALSE.
ir.pca <- prcomp(log.ir,center = TRUE, scale. = TRUE)
print(ir.pca)
# summary method
summary(ir.pca)
# plot method
plot(ir.pca, type = "l")
library(ggbiplot)
g <- ggbiplot(ir.pca, obs.scale = 1, var.scale = 1, groups = ir.species)
g <- g + scale_color_discrete(name = '')
g <- g + theme(legend.direction = 'horizontal', legend.position = 'top')
print(g)
g <- ggbiplot(ir.pca, obs.scale = 1, var.scale = 1, groups = ir.species, ellipse = TRUE, circle = TRUE)
g <- g + scale_color_discrete(name = '')
g <- g + theme(legend.direction = 'horizontal',legend.position = 'top')
print(g)
#ellipse: draw a normal data ellipse for each groupellipse.prob = 0.68, why?
g <- ggbiplot(ir.pca, obs.scale = 1, var.scale = 1,
groups = ir.species, ellipse = TRUE,
circle = TRUE)
g <- g + scale_color_discrete(name = '')
g <- g + theme(legend.direction = 'horizontal',
legend.position = 'top')
print(g)
# Predict PCs
predict(ir.pca, newdata=tail(log.ir, 2))
# Credit by thiagogm, https://www.r-bloggers.com/computing-and-visualizing-pca-in-r/
# Load data
data(iris)
head(iris, 3)
# log transform
log.ir <- log(iris[, 1:4])
ir.species <- iris[, 5]
# apply PCA - scale. = TRUE is highly advisable, but default is FALSE.
ir.pca <- prcomp(log.ir, center = TRUE, scale. = TRUE)
print(ir.pca)
# summary method
summary(ir.pca)
# plot method (scree plot)
plot(ir.pca, type = "l")
library(ggbiplot)
g <- ggbiplot(ir.pca, obs.scale = 1, var.scale = 1, groups = ir.species)
g <- g + scale_color_discrete(name = '')
g <- g + theme(legend.direction = 'horizontal', legend.position = 'top')
print(g)
# ellipse: draw a normal data ellipse for each group
# ellipse.prob = 0.68 (68% of data falls within +-1 SD)
g <- ggbiplot(ir.pca, obs.scale = 1, var.scale = 1,
groups = ir.species, ellipse = TRUE,
circle = TRUE)
g <- g + scale_color_discrete(name = '')
g <- g + theme(legend.direction = 'horizontal',
legend.position = 'top')
print(g)
# Predict PCs
predict(ir.pca, newdata=tail(log.ir, 4))
# Credit by thiagogm, https://www.r-bloggers.com/computing-and-visualizing-pca-in-r/
# Load data
data(iris)
head(iris, 3)
# log transform
log.ir <- log(iris[, 1:4])
ir.species <- iris[, 5]
# apply PCA - scale. = TRUE is highly advisable, but default is FALSE.
ir.pca <- prcomp(log.ir, center = TRUE, scale. = TRUE)
print(ir.pca)
# summary method
summary(ir.pca)
# plot method (scree plot)
plot(ir.pca, type = "l")
library(ggbiplot)
g <- ggbiplot(ir.pca, obs.scale = 1, var.scale = 1, groups = ir.species)
g <- g + scale_color_discrete(name = '')
g <- g + theme(legend.direction = 'horizontal', legend.position = 'top')
print(g)
# ellipse: draw a normal data ellipse for each group
# ellipse.prob = 0.68 (68% of data falls within +-1 SD)
g <- ggbiplot(ir.pca, obs.scale = 1, var.scale = 1,
groups = ir.species, ellipse = TRUE,
circle = TRUE)
g <- g + scale_color_discrete(name = '')
g <- g + theme(legend.direction = 'horizontal',
legend.position = 'top')
print(g)
# Predict PCs
predict(ir.pca, newdata=tail(log.ir, 2))
sum(ir.pca$rotation[1,]*ir.pca$rotation[1,])
View(ir.pca)
N = matrix(c(5, 18, 19, 12, 3, 7, 46, 29, 40, 7, 2, 20, 39, 49, 16),
nrow = 5,
dimnames = list(
"Level of education" = c("Some primary", "Primary completed", "Some secondary", "Secondary completed", "Some tertiary"),
"Category of readership" = c("Glance", "Fairly thorough", "Very thorough")))
View(N)
n = sum(N)
P = N/n
column.masses = colSums(P)
row.masses = rowSums(P)
# contingency table
N = matrix(c(5, 18, 19, 12, 3, 7, 46, 29, 40, 7, 2, 20, 39, 49, 16),
nrow = 5,
dimnames = list(
"Level of education" = c("Some primary", "Primary completed", "Some secondary", "Secondary completed", "Some tertiary"),
"Category of readership" = c("Glance", "Fairly thorough", "Very thorough")))
# compute observed proportions
n = sum(N)
P = N/n
# row and column masses
column.masses = colSums(P)
row.masses = rowSums(P)
# expected proportions E
# the value that we would expect to see under the assumption that
# there is no relationship between education and readership.
E = row.masses %o% column.masses
View(E)
# compute the residuals
R = (P - E)
View(R)
# This code is excerpted from the website:
# Understanding the Math of Correspondence Analysis
# https://www.displayr.com/math-correspondence-analysis/
# contingency table
N = matrix(c(5, 18, 19, 12, 3, 7, 46, 29, 40, 7, 2, 20, 39, 49, 16),
nrow = 5,
dimnames = list(
"Level of education" = c("Some primary", "Primary completed", "Some secondary", "Secondary completed", "Some tertiary"),
"Category of readership" = c("Glance", "Fairly thorough", "Very thorough")))
# compute observed proportions
n = sum(N) # n = sum of all entries in the table
P = N/n
# row and column masses
column.masses = colSums(P)
row.masses = rowSums(P)
# expected proportions E
# the value that we would expect to see under the assumption that
# there is no relationship between education and readership.
E = row.masses %o% column.masses
# residuals
R = (P - E)
# indexed residuals I
I = R / E
# singular values, eigenvalues & variance
Z = I * sqrt(E)
SVD = svd(Z)
rownames(SVD$u) = rownames(P)
rownames(SVD$v) = colnames(P)
eigenvalues = SVD$d^2
# standard coordinates
standard.coordinates.rows = sweep(SVD$u, 1, sqrt(row.masses), "/")
standard.coordinates.columns = sweep(SVD$v, 1, sqrt(column.masses), "/")
# principal coordinates
principal.coordinates.rows = sweep(standard.coordinates.rows, 2, SVD$d, "*")
principal.coordinates.columns = sweep(standard.coordinates.columns, 2, SVD$d, "*")
# quality
pc = rbind(principal.coordinates.rows, principal.coordinates.columns)
prop.table(pc ^ 2, 1)
View(pc)
# This code is excerpted from the website:
# Understanding the Math of Correspondence Analysis
# https://www.displayr.com/math-correspondence-analysis/
# contingency table
N = matrix(c(5, 18, 19, 12, 3, 7, 46, 29, 40, 7, 2, 20, 39, 49, 16),
nrow = 5,
dimnames = list(
"Level of education" = c("Some primary", "Primary completed", "Some secondary", "Secondary completed", "Some tertiary"),
"Category of readership" = c("Glance", "Fairly thorough", "Very thorough")))
# compute observed proportions
n = sum(N) # n = sum of all entries in the table
P = N/n
# row and column masses
column.masses = colSums(P)
row.masses = rowSums(P)
# expected proportions E
# the value that we would expect to see under the assumption that
# there is no relationship between education and readership.
E = row.masses %o% column.masses
# residuals
R = (P - E)
# indexed residuals I
I = R / E
# singular values, eigenvalues & variance
Z = I * sqrt(E)
SVD = svd(Z)
rownames(SVD$u) = rownames(P)
rownames(SVD$v) = colnames(P)
eigenvalues = SVD$d^2
# standard coordinates
standard.coordinates.rows = sweep(SVD$u, 1, sqrt(row.masses), "/")
standard.coordinates.columns = sweep(SVD$v, 1, sqrt(column.masses), "/")
# principal coordinates
principal.coordinates.rows = sweep(standard.coordinates.rows, 2, SVD$d, "*")
principal.coordinates.columns = sweep(standard.coordinates.columns, 2, SVD$d, "*")
# quality
pc = rbind(principal.coordinates.rows, principal.coordinates.columns)
prop.table(round(pc ^ 2, 3), 1)
prop.table = round((pc ^ 2, 1), 3)
# quality
pc = rbind(principal.coordinates.rows, principal.coordinates.columns)
# Calculate proportion table
prop_table = prop.table(pc ^ 2, 1)
# Round the results to three decimal places
rounded_prop_table = round(prop_table, 3)
# Display the rounded results
print(rounded_prop_table)
round(prop_table, 3)
# This code is excerpted from the website:
# Understanding the Math of Correspondence Analysis
# https://www.displayr.com/math-correspondence-analysis/
# contingency table
N = matrix(c(5, 18, 19, 12, 3, 7, 46, 29, 40, 7, 2, 20, 39, 49, 16),
nrow = 5,
dimnames = list(
"Level of education" = c("Some primary", "Primary completed", "Some secondary", "Secondary completed", "Some tertiary"),
"Category of readership" = c("Glance", "Fairly thorough", "Very thorough")))
# compute observed proportions
n = sum(N) # n = sum of all entries in the table
P = N/n
# row and column masses
column.masses = colSums(P)
row.masses = rowSums(P)
# expected proportions E
# the value that we would expect to see under the assumption that
# there is no relationship between education and readership.
E = row.masses %o% column.masses
# residuals
R = (P - E)
# indexed residuals I
I = R / E
# singular values, eigenvalues & variance
Z = I * sqrt(E)
SVD = svd(Z)
rownames(SVD$u) = rownames(P)
rownames(SVD$v) = colnames(P)
eigenvalues = SVD$d^2
# standard coordinates
standard.coordinates.rows = sweep(SVD$u, 1, sqrt(row.masses), "/")
standard.coordinates.columns = sweep(SVD$v, 1, sqrt(column.masses), "/")
# principal coordinates
principal.coordinates.rows = sweep(standard.coordinates.rows, 2, SVD$d, "*")
principal.coordinates.columns = sweep(standard.coordinates.columns, 2, SVD$d, "*")
# quality
pc = rbind(principal.coordinates.rows, principal.coordinates.columns)
# Calculate proportion table
prop_table = prop.table(pc ^ 2, 1)
# Round the results to three decimal places
round(prop_table, 3)
# Credit by thiagogm, https://www.r-bloggers.com/computing-and-visualizing-pca-in-r/
# Load data
data(iris)
head(iris, 3)
# log transform
log.ir <- log(iris[, 1:4])
ir.species <- iris[, 5]
# apply PCA - scale. = TRUE is highly advisable, but default is FALSE.
ir.pca <- prcomp(log.ir, center = TRUE, scale. = TRUE)
print(ir.pca)
# summary method
summary(ir.pca)
# plot method (scree plot)
plot(ir.pca, type = "l")
library(ggbiplot)
g <- ggbiplot(ir.pca, obs.scale = 1, var.scale = 1, groups = ir.species)
g <- g + scale_color_discrete(name = '')
g <- g + theme(legend.direction = 'horizontal', legend.position = 'top')
print(g)
# ellipse: draw a normal data ellipse for each group
# ellipse.prob = 0.68 (68% of data falls within +-1 SD)
g <- ggbiplot(ir.pca, obs.scale = 1, var.scale = 1,
groups = ir.species, ellipse = TRUE,
circle = TRUE)
g <- g + scale_color_discrete(name = '')
g <- g + theme(legend.direction = 'horizontal',
legend.position = 'top')
print(g)
sum(ir.pca$rotation[1,]*ir.pca$rotation[1,])
# Predict PCs
predict(ir.pca, newdata=tail(log.ir, 2))
#
# Convenience function for running iris example from section 6.3.2
# of Practical Data Science with R, Second Edition
#
# if this fails, make sure xgboost is installed:
# install.packages("xgboost")
library(xgboost)
#
# Input:
# - variable_matrix: matrix of input data
# - labelvec: numeric vector of class labels (1 is positive class)
#
# Returns:
# - xgboost model
#
fit_iris_example = function(variable_matrix, labelvec) {
cv = xgb.cv(variable_matrix, label = labelvec,
params=list(
objective="binary:logistic"
),
nfold=5,
nrounds=100,
print_every_n=10,
metrics="logloss")
evalframe = as.data.frame(cv$evaluation_log)
NROUNDS = which.min(evalframe$test_logloss_mean)
model = xgboost(data=variable_matrix, label=labelvec,
params=list(
objective="binary:logistic"
),
nrounds=NROUNDS,
verbose=FALSE)
model
}
iris <- iris
iris$class <- as.numeric(iris$Species == "setosa")
set.seed(2345)
intrain <- runif(nrow(iris)) < 0.75
train <- iris[intrain,]
test <- iris[!intrain,]
head(train)
##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species class
## 1          5.1         3.5          1.4         0.2  setosa     1
## 2          4.9         3.0          1.4         0.2  setosa     1
## 3          4.7         3.2          1.3         0.2  setosa     1
## 4          4.6         3.1          1.5         0.2  setosa     1
## 5          5.0         3.6          1.4         0.2  setosa     1
## 6          5.4         3.9          1.7         0.4  setosa     1
source("lime_iris_example.R")
input <- as.matrix(train[, 1:4])
model <- fit_iris_example(input, train$class)
library(lime)
explainer <- lime(train[,1:4],
model = model,
bin_continuous = TRUE,
n_bins = 10)
(example <- test[5, 1:4, drop=FALSE])
explanation <- lime::explain(example,
explainer,
n_labels = 1,
n_features = 4)
plot_features(explanation)
(example <- test[c(13, 24), 1:4])
test$class[c(13,24)]
explanation <- explain(example,
explainer,
n_labels = 1,
n_features = 4,
kernel_width = 0.5)
plot_features(explanation)
View(iris)
?source
intrain
type(intrain)
View(input)
View(test)
